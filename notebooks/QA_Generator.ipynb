{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answer Dataframe Creation\n",
    "\n",
    "In this phase, my objective is to construct a QA (Question-Answer) file using the LLM (LLaMA-3.0). This file will consist of question-answer pairs generated from a randomly selected subset of texts within my DataFrame. The primary purpose of this process is to prepare a robust dataset that supports my future experiments and evaluations effectively.\n",
    "\n",
    "**Steps to Create the QA File:**\n",
    "\n",
    "1.\tSelecting Random Texts:\\\n",
    "\t•\tTo ensure a diverse and representative sample of the dataset, I will randomly select texts from my DataFrame. This approach guarantees that the QA file covers a wide spectrum of topics and contexts, providing a comprehensive base for testing the model.\n",
    "2.\tGenerating Questions and Answers:\\\n",
    "\t•\tI will leverage the advanced natural language understanding capabilities of LLaMA-3.0 to generate meaningful and contextually relevant questions and their corresponding answers. This step is crucial as it tests the model’s ability to interpret and process textual information accurately.\n",
    "3.\tFormatting the QA File:\\\n",
    "\t•\tEach question-answer pair will be systematically organized into a structured file. Entries will include the ID of the original text, the generated question, and its answer. This format will facilitate easy access and manipulation of data for subsequent evaluation steps.\n",
    "4.\tEvaluating the QA Dataframe:\\\n",
    "\t•\tAfter creation, the QA dataframe will undergo a rigorous evaluation to ensure that the generated questions meet my criteria of clarity, reality, and specificity. This evaluation will be pivotal in determining the usability of the generated questions in real-world scenarios and further experimental setups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.konchakova/Thesis/huber/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/s.konchakova/Thesis/huber/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-08-30 01:41:27,632 - INFO - Use pytorch device_name: mps\n",
      "2024-08-30 01:41:27,633 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Langchain and embedding imports\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Local imports from script directories\n",
    "cwd = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(cwd, '../assets')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(cwd, '../src')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(cwd, '../')))\n",
    "\n",
    "from scripts.pinecone_func import pinecone_upsert\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Environment variables setup\n",
    "load_dotenv()\n",
    "\n",
    "# Instance of embedding model\n",
    "embed_model = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Initialising LLama Model\n",
    "\n",
    "In this section, I describe how I set up the lightweight LLaMA 3.0 model for running experiments on my local machine. This configuration aims to generate high-quality outputs while efficiently managing computational resources.\n",
    "\n",
    "**Model Initialization:**\\\n",
    "\t•\tModel Path: The model files are loaded from a local directory.\\\n",
    "\t•\tTemperature: Set at 0.5 to generate more precise and less random outputs.\\\n",
    "\t•\tMax Tokens: Increased to 512, allowing the model to generate longer responses.\\\n",
    "\t•\tTop_p and Top_k: Configured to control the generation’s randomness, focusing the model to select the most likely next words.\\\n",
    "\t•\tRepeat Penalty: Increased to 1.2 to discourage repetitive responses, enhancing the variety and relevance of the generated content.\\\n",
    "\t•\tGPU Layers: Utilizing one GPU layer to optimize the use of available hardware resources without compromising performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the LlamaCpp model\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../models/Meta-Llama-3-8B-Instruct.Q3_K_L.gguf\",\n",
    "    temperature=0.5,  # Reduced temperature for more focused outputs\n",
    "    max_tokens=512,  # Increased max tokens to allow for longer responses\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    n_ctx=8192,\n",
    "    repeat_penalty=1.2,  # Increased repeat penalty to discourage repetition\n",
    "    n_gpu_layers=1, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating QA dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Preparation\n",
    "\n",
    "This section outlines the process of preparing the underlying data from which the QA pairs will be generated. The data preparation is crucial for ensuring the quality and variability of the input data, which directly influences the quality of the QA output.\n",
    "\n",
    "Steps Involved in Data Preparation:\n",
    "\n",
    "1.\tData Loading:\\\n",
    "    •\tThe complete dataset is loaded from a CSV file to ensure all available information is included for processing. This step captures the initial scope of the data available for generating QA pairs.\n",
    "2.\tData Cleaning:\\\n",
    "    •\tI clean the dataset by removing any rows where the text column contains NA values. This step is crucial because NA values can occur during text extraction due to language mismatches or missing entries. Cleaning ensures the dataset’s integrity, providing a solid foundation for generating accurate and relevant QA pairs.\n",
    "3.\tSelecting a Random Subset:\\\n",
    "    •\tI select a random subset of 50 entries from the cleaned dataset. This subset is not only manageable in size for in-depth processing but also diverse, encompassing a range of contexts and topics. This diversity is vital for enhancing the robustness and comprehensiveness of the QA training and evaluation phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape of the dataframe: (1034, 7)\n",
      "Shape of the dataframe after dropping na values: (893, 7)\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "data_full = pd.read_csv(\"../assets/csv/data_full.csv\", index_col=0)\n",
    "print(f'Initial shape of the dataframe: {data_full.shape}')\n",
    "\n",
    "#dropping na values\n",
    "data_full = data_full.dropna(subset=['text'])\n",
    "print(f'Shape of the dataframe after dropping na values: {data_full.shape}')\n",
    "\n",
    "#selecting random subset\n",
    "df_subset = data_full.sample(50, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### QA Pair Extraction and Generation\n",
    "\n",
    "This part of the process involves extracting and generating QA pairs from the dataset using predefined patterns and an LLM-based generation method. The aim is to create a diverse and comprehensive QA dataset suitable for evaluating the LLaMA model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps Involved in QA Pair Generation:**\n",
    "\n",
    "1.\tQA Pair Extraction Function:\\\n",
    "\t•\tThis function searches through the provided text for predefined patterns indicating a question-answer format. It first looks for explicit pairs marked with “Question:” and “Answer:”. If none are found, it tries to identify standalone questions marked by a question mark and treats the following text as the answer.\n",
    "2. \tSingle Question Generation Function:\\\n",
    "\t•\tThis function prepares a text input for the LLM, truncating it to manage size and ensure focus. It formats a prompt asking the LLM to generate one specific question and its answer based on the truncated text, adhering to defined rules such as using specific terms and ensuring relevance.\\\n",
    "\t•\tAttempts to generate a QA pair are made, with retries if the initial attempts fail, to manage errors or unsatisfactory responses.\n",
    "3. Batch QA Pair Generation Function:\\\n",
    "\t•\tOperating on the entire DataFrame, this function attempts to generate multiple QA pairs. It shuffles the DataFrame to randomize the starting point, processes each row individually, and uses the generate_single_question function to produce QA pairs.\\\n",
    "\t•\tIt logs the process, captures successful QA pairs, and stops once the desired number of pairs is generated or the DataFrame is fully processed.\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qa_pair(text):\n",
    "    \"\"\"\n",
    "    Extracts a question-answer pair from a given text. First, it looks for explicitly formatted Q&A pairs.\n",
    "    If none are found, it searches for any sentence ending with a question mark and treats the following\n",
    "    text as the answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The text from which to extract the QA pair.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the question and answer as strings, or (None, None) if no pair is found.\n",
    "    \"\"\"\n",
    "    qa_pattern = re.compile(r'(?:Question:|Q:)\\s*(.*?)\\s*(?:Answer:|A:)\\s*(.*)', re.DOTALL | re.IGNORECASE)\n",
    "    match = qa_pattern.search(text)\n",
    "    \n",
    "    if match:\n",
    "        question = match.group(1).strip()\n",
    "        answer = match.group(2).strip()\n",
    "        return question, answer\n",
    "    \n",
    "    # If no Q&A pattern found, try to extract any sentence with a question mark\n",
    "    question_pattern = re.compile(r'([^.!?]+\\?)')\n",
    "    questions = question_pattern.findall(text)\n",
    "    \n",
    "    if questions:\n",
    "        question = questions[0].strip()\n",
    "        # Use the rest of the text as the answer\n",
    "        answer = text[text.index(question) + len(question):].strip()\n",
    "        return question, answer\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def generate_single_question(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Generates a single question-answer pair from a provided text using an LLM model. It retries generation\n",
    "    up to a specified number of attempts if the initial tries fail.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text from which to generate the question.\n",
    "    - max_retries (int): The maximum number of retries allowed for generating the QA pair.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list containing a single tuple of the generated question and answer, or an empty list if generation fails.\n",
    "    \"\"\"\n",
    "    max_text_length = 300\n",
    "    truncated_text = ' '.join(text.split()[:max_text_length])\n",
    "    \n",
    "    template = \"\"\"Text: {text}\n",
    "\n",
    "Generate 1 specific question and answer based on the text above. Follow these rules:\n",
    "\n",
    "1. Use specific terms, names, and figures from the text in your question.\n",
    "2. The question must be directly related to the text content.\n",
    "3. The answer should be comprehensive and use information from the text.\n",
    "4. Use only this format, with no additional text:\n",
    "\n",
    "Q: [Specific question]\n",
    "A: [Detailed answer]\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    formatted_prompt = prompt.format(text=truncated_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = llm.invoke(formatted_prompt)\n",
    "            logging.info(f\"Raw LLM response: {response}\")\n",
    "\n",
    "            question, answer = extract_qa_pair(response)\n",
    "            \n",
    "            if question and answer:\n",
    "                return [(question, answer)]\n",
    "            else:\n",
    "                logging.warning(f\"Attempt {attempt + 1}: Failed to extract Q&A. Retrying...\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt + 1}: Error in generate_single_question: {str(e)}\")\n",
    "    \n",
    "    logging.error(f\"Failed to generate Q&A after {max_retries} attempts. Problematic text: {truncated_text[:100]}...\")\n",
    "    return []\n",
    "\n",
    "def generate_qa_pairs(df: pd.DataFrame, max_questions: int = 5) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Generates a list of question-answer pairs from a DataFrame containing text data. This function shuffles\n",
    "    the DataFrame to ensure diverse starting points and processes each row until the desired number of QA\n",
    "    pairs is generated or the DataFrame is fully traversed.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the text data from which to generate QA pairs.\n",
    "    - max_questions (int): The maximum number of QA pairs to generate.\n",
    "\n",
    "    Returns:\n",
    "    - List[Tuple[str, str, str]]: A list of tuples, each containing the document ID, question, and answer.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    all_qa_pairs = []\n",
    "    processed_ids = set()\n",
    "\n",
    "    # Shuffle the dataframe to ensure we're not always starting from the same place\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if len(all_qa_pairs) >= max_questions:\n",
    "            break\n",
    "\n",
    "        if row['id'] in processed_ids:\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing row with ID: {row['id']}\")\n",
    "        qa_pairs = generate_single_question(row['text'])\n",
    "        \n",
    "        if qa_pairs:\n",
    "            for question, answer in qa_pairs:\n",
    "                all_qa_pairs.append((row['id'], question, answer))\n",
    "                logging.info(f\"Added QA pair for ID {row['id']}: Q: {question[:50]}... A: {answer[:50]}...\")\n",
    "                if len(all_qa_pairs) >= max_questions:\n",
    "                    break\n",
    "            processed_ids.add(row['id'])\n",
    "        else:\n",
    "            logging.warning(f\"No QA pairs generated for row with ID: {row['id']}\")\n",
    "\n",
    "        if len(all_qa_pairs) >= max_questions:\n",
    "            break\n",
    "\n",
    "    logging.info(f\"Total QA pairs generated: {len(all_qa_pairs)}\")\n",
    "    return all_qa_pairs\n",
    "\n",
    "def clean_evaluated_questions(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and formats the 'question' and 'answer' columns in a DataFrame by removing specific unwanted\n",
    "    substrings, characters, and formatting cues. It also handles special cases like removing everything\n",
    "    after certain patterns and trimming extra spaces.\n",
    "\n",
    "    Parameters:\n",
    "    - df_path (str): Path to the CSV file containing the evaluation data.\n",
    "    - output_path (str): Path where the cleaned CSV will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame with updated 'question' and 'answer' columns.\n",
    "    \"\"\"\n",
    "    # Load the DataFrame from the specified path\n",
    "    evaluated_df = input_df.copy()\n",
    "\n",
    "    # Define a helper function to apply various cleaning operations to a column\n",
    "    def clean_column(text: str) -> str:\n",
    "        # Remove specific phrases and extra new lines\n",
    "        replacements = {\n",
    "            'Question\\n\\n': '', 'Question\\n': '', 'Question:': '',\n",
    "            'Answer\\n\\n': '', 'Answer\\n': '', 'Answer:': '',\n",
    "            'Q:': '', 'A:': ''\n",
    "        }\n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Remove content after certain patterns\n",
    "        for delimiter in ['\\n\\n**', '\\n\\n', '\\n']:\n",
    "            if delimiter in text:\n",
    "                text = text.split(delimiter)[0]\n",
    "        \n",
    "        # Strip leading and trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    # Clean 'question' and 'answer' columns\n",
    "    evaluated_df['question'] = evaluated_df['question'].apply(clean_column)\n",
    "    evaluated_df['answer'] = evaluated_df['answer'].apply(clean_column)\n",
    "\n",
    "\n",
    "    # Return the cleaned DataFrame\n",
    "    return evaluated_df\n",
    "\n",
    "# Generate QA pairs\n",
    "qa_pairs = generate_qa_pairs(data_full, max_questions=75)\n",
    "\n",
    "# Print the generated QA pairs\n",
    "for doc_id, question, answer in qa_pairs:\n",
    "    print(f\"Document ID: {doc_id}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Convert to a dataframe\n",
    "qa_df = pd.DataFrame(qa_pairs, columns=['id', 'question', 'answer'])\n",
    "qa_df = clean_evaluated_questions(qa_df)\n",
    "print(qa_df)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "qa_df.to_csv('qa_pairs_to_evaluate.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## QA Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After generating the question and answer pairs, a rigorous evaluation is essential to ensure their quality and applicability. This evaluation is structured around three key criteria:\n",
    "\n",
    "1.\tSpecificity: Questions should be precise rather than broad. For instance, rather than asking a vague question like, “What is this article about?” it is more beneficial to ask something specific, such as, “What is the main argument presented in article ‘XYZ’?” This approach ensures that the questions probe specific knowledge areas and are directly related to the text.\n",
    "2.\tRealism: The questions must reflect queries that students are likely to pose in an academic context, embodying practical, real-world relevance. This means they should be formulated in a way that naturally aligns with typical student inquiries and educational standards.\n",
    "3.\tClarity: It is critical that each question is articulated clearly, without any ambiguous terms or confusing phrasing that could mislead or perplex students. Clarity in question formulation is vital to effective assessment and learning.\n",
    "\n",
    "The evaluation comprises two distinct phases:\n",
    "\n",
    "1.\tCross-Evaluation with an Alternate Large Language Model: This step involves using another sophisticated language model to verify the originality and accuracy of the generated questions, providing an automated layer of quality assurance.\n",
    "2.\tManual Review with Human-in-the-Loop: The final stage of the evaluation process involves a manual review by an expert. This human element ensures that the questions not only meet technical standards but also resonate with human judgment and educational value.\n",
    "\n",
    "This dual-stage evaluation strategy aims to establish the reliability and efficiency of using AI-generated content in educational settings, potentially paving the way for broader applications in future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.read_csv('../assets/csv/qa_pairs_to_evaluate.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/75 [00:00<?, ?it/s]2024-08-30 13:50:56,299 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   1%|▏         | 1/75 [00:14<18:09, 14.72s/it]2024-08-30 13:51:11,625 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   3%|▎         | 2/75 [00:30<18:32, 15.24s/it]2024-08-30 13:51:27,048 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   4%|▍         | 3/75 [00:47<19:11, 16.00s/it]2024-08-30 13:51:43,856 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   5%|▌         | 4/75 [01:03<19:08, 16.17s/it]2024-08-30 13:52:00,575 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   7%|▋         | 5/75 [01:13<16:01, 13.74s/it]2024-08-30 13:52:09,865 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   8%|▊         | 6/75 [01:28<16:22, 14.24s/it]2024-08-30 13:52:25,012 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   9%|▉         | 7/75 [01:42<16:13, 14.31s/it]2024-08-30 13:52:39,155 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  11%|█         | 8/75 [01:50<13:37, 12.20s/it]2024-08-30 13:52:45,397 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:52:45,399 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:53:07,515 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  12%|█▏        | 9/75 [02:25<21:22, 19.43s/it]2024-08-30 13:53:22,569 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  13%|█▎        | 10/75 [02:36<18:09, 16.76s/it]2024-08-30 13:53:31,308 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:53:31,310 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:53:54,987 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  15%|█▍        | 11/75 [03:11<23:53, 22.39s/it]2024-08-30 13:54:08,238 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  16%|█▌        | 12/75 [03:21<19:19, 18.41s/it]2024-08-30 13:54:17,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  17%|█▋        | 13/75 [03:36<18:02, 17.47s/it]2024-08-30 13:54:31,105 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:54:31,106 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:54:53,090 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  19%|█▊        | 14/75 [04:13<23:44, 23.36s/it]2024-08-30 13:55:10,703 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  20%|██        | 15/75 [04:24<19:48, 19.81s/it]2024-08-30 13:55:21,561 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  21%|██▏       | 16/75 [04:37<17:26, 17.74s/it]2024-08-30 13:55:32,590 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:55:32,595 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:55:54,838 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  23%|██▎       | 17/75 [05:12<21:57, 22.72s/it]2024-08-30 13:56:08,970 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  24%|██▍       | 18/75 [05:28<19:46, 20.81s/it]2024-08-30 13:56:25,259 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  25%|██▌       | 19/75 [05:42<17:28, 18.72s/it]2024-08-30 13:56:38,788 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  27%|██▋       | 20/75 [05:49<14:05, 15.37s/it]2024-08-30 13:56:44,640 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:56:44,643 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:57:06,422 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  28%|██▊       | 21/75 [06:20<17:57, 19.96s/it]2024-08-30 13:57:17,476 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  29%|██▉       | 22/75 [06:30<14:55, 16.89s/it]2024-08-30 13:57:25,128 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:57:25,130 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:57:47,274 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  31%|███       | 23/75 [07:03<18:46, 21.67s/it]2024-08-30 13:57:59,879 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  32%|███▏      | 24/75 [07:14<15:44, 18.51s/it]2024-08-30 13:58:09,073 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:58:09,076 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:58:31,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  33%|███▎      | 25/75 [07:48<19:26, 23.33s/it]2024-08-30 13:58:45,193 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  35%|███▍      | 26/75 [07:58<15:40, 19.19s/it]2024-08-30 13:58:53,067 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:58:53,069 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 13:59:15,337 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  36%|███▌      | 27/75 [08:30<18:28, 23.09s/it]2024-08-30 13:59:27,359 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  37%|███▋      | 28/75 [08:41<15:14, 19.45s/it]2024-08-30 13:59:37,922 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  39%|███▊      | 29/75 [08:55<13:41, 17.86s/it]2024-08-30 13:59:50,562 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 13:59:50,564 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:00:13,277 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  40%|████      | 30/75 [09:30<17:11, 22.91s/it]2024-08-30 14:00:27,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  41%|████▏     | 31/75 [09:43<14:39, 19.98s/it]2024-08-30 14:00:40,103 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  43%|████▎     | 32/75 [09:59<13:27, 18.78s/it]2024-08-30 14:00:54,200 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:00:54,202 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:01:16,787 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  44%|████▍     | 33/75 [10:31<16:00, 22.87s/it]2024-08-30 14:01:28,664 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  45%|████▌     | 34/75 [10:42<13:13, 19.35s/it]2024-08-30 14:01:39,717 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  47%|████▋     | 35/75 [10:58<12:04, 18.11s/it]2024-08-30 14:01:52,976 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:01:52,979 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:02:15,460 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  48%|████▊     | 36/75 [11:35<15:30, 23.85s/it]2024-08-30 14:02:32,334 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  49%|████▉     | 37/75 [11:49<13:18, 21.02s/it]2024-08-30 14:02:46,383 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  51%|█████     | 38/75 [11:58<10:42, 17.37s/it]2024-08-30 14:02:53,479 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:02:53,480 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:03:15,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  52%|█████▏    | 39/75 [12:32<13:20, 22.24s/it]2024-08-30 14:03:28,717 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  53%|█████▎    | 40/75 [12:44<11:14, 19.26s/it]2024-08-30 14:03:41,757 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  55%|█████▍    | 41/75 [12:56<09:43, 17.15s/it]2024-08-30 14:03:51,616 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:03:51,617 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:04:13,834 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  56%|█████▌    | 42/75 [13:27<11:41, 21.25s/it]2024-08-30 14:04:24,069 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  57%|█████▋    | 43/75 [13:38<09:43, 18.23s/it]2024-08-30 14:04:33,706 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:04:33,707 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:04:55,679 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  59%|█████▊    | 44/75 [14:11<11:40, 22.59s/it]2024-08-30 14:05:08,310 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  60%|██████    | 45/75 [14:27<10:19, 20.65s/it]2024-08-30 14:05:24,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  61%|██████▏   | 46/75 [14:38<08:36, 17.81s/it]2024-08-30 14:05:33,662 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:05:33,663 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:05:55,620 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  63%|██████▎   | 47/75 [15:15<10:52, 23.30s/it]2024-08-30 14:06:11,653 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  64%|██████▍   | 48/75 [15:25<08:48, 19.56s/it]2024-08-30 14:06:22,538 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  65%|██████▌   | 49/75 [15:36<07:21, 17.00s/it]2024-08-30 14:06:31,679 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:06:31,680 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:06:54,123 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  67%|██████▋   | 50/75 [16:11<09:14, 22.20s/it]2024-08-30 14:07:07,692 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  68%|██████▊   | 51/75 [16:18<07:03, 17.65s/it]2024-08-30 14:07:12,989 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:07:12,991 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:07:35,374 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  69%|██████▉   | 52/75 [16:52<08:43, 22.75s/it]2024-08-30 14:07:49,196 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  71%|███████   | 53/75 [17:05<07:11, 19.62s/it]2024-08-30 14:08:02,138 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  72%|███████▏  | 54/75 [17:14<05:48, 16.59s/it]2024-08-30 14:08:09,496 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:08:09,496 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:08:31,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  73%|███████▎  | 55/75 [17:47<07:08, 21.41s/it]2024-08-30 14:08:44,325 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  75%|███████▍  | 56/75 [18:00<06:00, 19.00s/it]2024-08-30 14:08:57,649 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  76%|███████▌  | 57/75 [18:16<05:25, 18.10s/it]2024-08-30 14:09:11,505 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:09:11,507 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:09:33,736 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  77%|███████▋  | 58/75 [18:45<06:00, 21.21s/it]2024-08-30 14:09:41,879 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  79%|███████▊  | 59/75 [19:01<05:16, 19.75s/it]2024-08-30 14:09:58,259 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  80%|████████  | 60/75 [19:15<04:30, 18.03s/it]2024-08-30 14:10:10,345 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:10:10,348 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:10:32,929 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  81%|████████▏ | 61/75 [19:50<05:23, 23.09s/it]2024-08-30 14:10:47,557 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  83%|████████▎ | 62/75 [20:01<04:13, 19.50s/it]2024-08-30 14:10:58,578 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  84%|████████▍ | 63/75 [20:13<03:26, 17.17s/it]2024-08-30 14:11:08,251 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:11:08,253 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:11:30,069 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  85%|████████▌ | 64/75 [20:47<04:05, 22.28s/it]2024-08-30 14:11:43,996 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  87%|████████▋ | 65/75 [20:58<03:09, 19.00s/it]2024-08-30 14:11:53,714 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:11:53,716 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:12:15,645 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  88%|████████▊ | 66/75 [21:30<03:26, 22.91s/it]2024-08-30 14:12:27,751 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  89%|████████▉ | 67/75 [21:43<02:38, 19.84s/it]2024-08-30 14:12:40,101 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  91%|█████████ | 68/75 [21:52<01:55, 16.52s/it]2024-08-30 14:12:47,132 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:12:47,134 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:13:09,193 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  92%|█████████▏| 69/75 [22:21<02:01, 20.17s/it]2024-08-30 14:13:16,462 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  93%|█████████▎| 70/75 [22:28<01:21, 16.36s/it]2024-08-30 14:13:23,413 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:13:23,416 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:13:45,546 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  95%|█████████▍| 71/75 [23:06<01:30, 22.71s/it]2024-08-30 14:14:00,846 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:14:00,849 - INFO - Retrying request to /chat/completions in 0.810610 seconds\n",
      "2024-08-30 14:14:01,834 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:14:01,836 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:14:23,752 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  96%|█████████▌| 72/75 [23:44<01:21, 27.32s/it]2024-08-30 14:14:38,903 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:14:38,905 - INFO - Retrying request to /chat/completions in 0.822989 seconds\n",
      "2024-08-30 14:14:39,931 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:14:39,934 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:15:00,253 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:00,256 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:15:00,962 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:00,964 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:15:21,315 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:21,317 - INFO - Retrying request to /chat/completions in 1.868188 seconds\n",
      "2024-08-30 14:15:23,381 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:23,384 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:15:23,754 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:23,756 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:15:44,089 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:44,093 - INFO - Retrying request to /chat/completions in 1.785702 seconds\n",
      "2024-08-30 14:15:46,063 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:46,066 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:15:46,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:15:46,880 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:16:07,127 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:16:07,129 - INFO - Retrying request to /chat/completions in 1.640203 seconds\n",
      "2024-08-30 14:16:09,622 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:16:09,624 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:16:16,280 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:16:16,281 - INFO - Retrying request to /chat/completions in 0.766324 seconds\n",
      "2024-08-30 14:16:17,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:16:17,236 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:16:37,493 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:16:37,495 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:16:51,403 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:16:51,405 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:17:11,667 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:17:11,669 - INFO - Retrying request to /chat/completions in 1.583347 seconds\n",
      "2024-08-30 14:17:13,459 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:17:13,461 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:17:27,733 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:17:27,735 - INFO - Retrying request to /chat/completions in 0.857975 seconds\n",
      "2024-08-30 14:17:28,776 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:17:28,778 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:17:49,058 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:17:49,060 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:18:21,449 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:18:21,451 - INFO - Retrying request to /chat/completions in 0.760608 seconds\n",
      "2024-08-30 14:18:22,406 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:18:22,409 - INFO - Retrying request to /chat/completions in 1.568274 seconds\n",
      "2024-08-30 14:18:24,162 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:18:24,165 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:19:31,522 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:19:31,527 - INFO - Retrying request to /chat/completions in 0.900505 seconds\n",
      "2024-08-30 14:19:32,636 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:19:32,637 - INFO - Retrying request to /chat/completions in 1.737185 seconds\n",
      "2024-08-30 14:19:34,574 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:19:34,577 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:20:23,595 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:20:23,598 - INFO - Retrying request to /chat/completions in 0.864278 seconds\n",
      "2024-08-30 14:20:24,658 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:20:24,659 - INFO - Retrying request to /chat/completions in 1.744419 seconds\n",
      "2024-08-30 14:20:26,673 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:20:26,678 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:21:16,670 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:21:16,673 - INFO - Retrying request to /chat/completions in 0.835558 seconds\n",
      "2024-08-30 14:21:17,861 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:21:17,863 - INFO - Retrying request to /chat/completions in 1.986161 seconds\n",
      "2024-08-30 14:21:20,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:21:20,032 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:21:41,709 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  97%|█████████▋| 73/75 [30:57<04:57, 149.00s/it]2024-08-30 14:21:51,888 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:21:51,891 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:22:12,173 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:22:12,175 - INFO - Retrying request to /chat/completions in 1.801255 seconds\n",
      "2024-08-30 14:22:14,158 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:22:14,161 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:22:14,714 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:22:14,717 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:22:34,982 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:22:34,984 - INFO - Retrying request to /chat/completions in 1.520352 seconds\n",
      "2024-08-30 14:22:36,698 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:22:36,700 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:22:38,549 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:22:38,553 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:22:58,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:22:58,811 - INFO - Retrying request to /chat/completions in 1.629809 seconds\n",
      "2024-08-30 14:23:00,656 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:23:00,659 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:23:03,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:23:03,841 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:23:24,210 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:23:24,213 - INFO - Retrying request to /chat/completions in 1.665312 seconds\n",
      "2024-08-30 14:23:26,066 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:23:26,069 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:23:29,620 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:23:29,622 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:23:49,863 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:23:49,865 - INFO - Retrying request to /chat/completions in 1.723031 seconds\n",
      "2024-08-30 14:23:51,968 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:23:51,970 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:24:03,514 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:24:03,516 - INFO - Retrying request to /chat/completions in 0.793855 seconds\n",
      "2024-08-30 14:24:04,489 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:24:04,492 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:24:24,764 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:24:24,767 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:24:32,352 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:24:32,355 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:24:52,600 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:24:52,602 - INFO - Retrying request to /chat/completions in 1.931729 seconds\n",
      "2024-08-30 14:24:54,731 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:24:54,734 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:25:57,271 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:25:57,274 - INFO - Retrying request to /chat/completions in 0.893584 seconds\n",
      "2024-08-30 14:25:58,355 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:25:58,356 - INFO - Retrying request to /chat/completions in 1.737696 seconds\n",
      "2024-08-30 14:26:00,284 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:26:00,285 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:27:18,891 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:27:18,894 - INFO - Retrying request to /chat/completions in 0.962330 seconds\n",
      "2024-08-30 14:27:20,047 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:27:20,050 - INFO - Retrying request to /chat/completions in 1.544187 seconds\n",
      "2024-08-30 14:27:21,794 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:27:21,796 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:28:55,491 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  99%|█████████▊| 74/75 [38:15<03:55, 235.82s/it]2024-08-30 14:29:10,208 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:10,211 - INFO - Retrying request to /chat/completions in 0.823911 seconds\n",
      "2024-08-30 14:29:11,228 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:11,230 - INFO - Retrying request to /chat/completions in 1.649766 seconds\n",
      "2024-08-30 14:29:13,072 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:13,077 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:29:13,583 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:13,586 - INFO - Retrying request to /chat/completions in 0.865823 seconds\n",
      "2024-08-30 14:29:14,641 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:14,642 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:29:34,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:34,918 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:29:35,186 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:35,189 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:29:55,441 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:55,442 - INFO - Retrying request to /chat/completions in 1.797093 seconds\n",
      "2024-08-30 14:29:57,427 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:57,429 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:29:57,967 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:29:57,969 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:30:18,270 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:30:18,272 - INFO - Retrying request to /chat/completions in 1.928596 seconds\n",
      "2024-08-30 14:30:20,384 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:30:20,387 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:30:20,712 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:30:20,714 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:30:41,053 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:30:41,055 - INFO - Retrying request to /chat/completions in 1.678862 seconds\n",
      "2024-08-30 14:30:42,920 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:30:42,924 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:30:52,912 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:30:52,914 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:31:13,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:31:13,235 - INFO - Retrying request to /chat/completions in 1.944391 seconds\n",
      "2024-08-30 14:31:15,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:31:15,374 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:31:46,459 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:31:46,461 - INFO - Retrying request to /chat/completions in 0.991496 seconds\n",
      "2024-08-30 14:31:47,691 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:31:47,693 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:32:07,983 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:32:07,985 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:32:38,257 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:32:38,259 - INFO - Retrying request to /chat/completions in 0.787384 seconds\n",
      "2024-08-30 14:32:39,242 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:32:39,245 - INFO - Retrying request to /chat/completions in 1.859740 seconds\n",
      "2024-08-30 14:32:41,301 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:32:41,304 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:34:39,609 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:34:39,611 - INFO - Retrying request to /chat/completions in 0.936884 seconds\n",
      "2024-08-30 14:34:40,741 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:34:40,743 - INFO - Retrying request to /chat/completions in 1.945271 seconds\n",
      "2024-08-30 14:34:42,886 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:34:42,889 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:34:45,189 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:34:45,192 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:35:05,467 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:35:05,469 - INFO - Retrying request to /chat/completions in 1.705017 seconds\n",
      "2024-08-30 14:35:07,347 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:35:07,349 - WARNING - Rate limit reached. Retrying...\n",
      "2024-08-30 14:35:36,156 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:35:36,158 - INFO - Retrying request to /chat/completions in 0.852115 seconds\n",
      "2024-08-30 14:35:37,196 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-30 14:35:37,199 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-08-30 14:35:59,084 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions: 100%|██████████| 75/75 [45:09<00:00, 36.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  id  \\\n",
      "0   29d3d7b9e48f046d75821fe3b763da05   \n",
      "1   d804b24d051b1c5f06d0e0819f25ca37   \n",
      "2   d3251ee436c00c1107189675b4bd8fc7   \n",
      "3   e8de7463b8dde36dadddab419c363a57   \n",
      "4   ba886c5587fbbd26b9020e0c258a8ca7   \n",
      "..                               ...   \n",
      "70  4cac46414d07fba4f66281e2f625418f   \n",
      "71  c001c31186c9b7275a1917594b89926e   \n",
      "72  10b5c43940a1595f5e955ee4fdb4fd93   \n",
      "73  6621820746cd195d905c3bf7ab37006a   \n",
      "74  86d2b2d0bfd932492ba0d6ceb4c5e252   \n",
      "\n",
      "                                             question  \\\n",
      "0    What are some potential benefits of a new cur...   \n",
      "1    How will students learn about different metho...   \n",
      "2    How will the research group DeSBi improve sta...   \n",
      "3   What are some of the key findings in \"Debt Mat...   \n",
      "4   What is the main focus of Prof. Dr. Joachim Ga...   \n",
      "..                                                ...   \n",
      "70        What are Jiawei Zhang's research interests?   \n",
      "71        What was Hedda Nielsen's role in 2018-2019?   \n",
      "72   What are some of the research interests of Ch...   \n",
      "73  How does the adaptive step-length (ASL) approa...   \n",
      "74  What is the main research focus of Prof. Dr. r...   \n",
      "\n",
      "                                               answer Specificity Realism  \\\n",
      "0    According to Bahaj and Reis (2023), one poten...           2       4   \n",
      "1    After participating in this course, students ...           3       4   \n",
      "2    The research group DeSBi aims to develop nove...           4       5   \n",
      "3   According to the study, one of the key finding...           5       4   \n",
      "4   The main focus of Prof. Dr. Joachim Gassen is ...           3       4   \n",
      "..                                                ...         ...     ...   \n",
      "70  According to the provided CV, Jiawei Zhang's r...           4       5   \n",
      "71  According to her CV, Hedda Nielsen served as R...           5       5   \n",
      "72   Christian Wey's research interests include ma...           3       4   \n",
      "73  The ASL approach compares favorably with penal...           4       5   \n",
      "74  Her main research focus lies in developing kno...           5       5   \n",
      "\n",
      "   Clarity Average Score Is Non-Specific  \n",
      "0        3           3.0           False  \n",
      "1        4      3.666667           False  \n",
      "2        4      4.333333           False  \n",
      "3        5      4.666667           False  \n",
      "4        4      3.666667           False  \n",
      "..     ...           ...             ...  \n",
      "70       5      4.666667           False  \n",
      "71       5           5.0           False  \n",
      "72       3      3.333333           False  \n",
      "73       4      4.333333           False  \n",
      "74       5           5.0           False  \n",
      "\n",
      "[75 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = openai.Client(api_key=api_key)\n",
    "\n",
    "def is_non_specific_question(question):\n",
    "    # List of patterns for very general, non-specific questions\n",
    "    patterns = [\n",
    "        r\"^what is this about\\??$\",\n",
    "        r\"^what is the main topic\\??$\",\n",
    "        r\"^what (is|are) the main point(s)?\\??$\",\n",
    "        r\"^can you summarize this\\??$\",\n",
    "        r\"^what (is|are) the key takeaway(s)?\\??$\",\n",
    "        r\"^what is the main (point|idea|topic) of this (article|paper)\\??$\",\n",
    "        r\"^what information does the text provide\\??$\",\n",
    "        r\"^what is the main point of this paper\\??$\",\n",
    "        r\"^[Your specific question here]\\??$\",  # Placeholder for additional patterns\n",
    "        # Patterns for detecting questions that refer to unspecified models or papers\n",
    "        r\"^what (does|do) the (model|paper|study) (propose|suggest|conclude)\\??$\",\n",
    "        r\"^how does the (model|paper|study) improve (on|over) existing (models|methods)\\??$\"\n",
    "    ]\n",
    "\n",
    "    # Convert the input question to lower case for case insensitive matching\n",
    "    question = question.lower()\n",
    "    \n",
    "    # Check if the question matches any pattern\n",
    "    return any(re.match(pattern, question) for pattern in patterns)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=120), stop=stop_after_attempt(20), retry=retry_if_exception_type((openai.RateLimitError, openai.APIError)))\n",
    "def evaluate_question(question):\n",
    "    # Check if it's a non-specific question\n",
    "    if is_non_specific_question(question):\n",
    "        return {\n",
    "            \"Specificity\": 1,  # Very low specificity\n",
    "            \"Realism\": 1,     # Likely realistic, as these are common questions\n",
    "            \"Clarity\": 1      # Usually very clear, even if not specific\n",
    "        }\n",
    "\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Evaluate the following question based on three criteria using a scale of 1 to 5:\n",
    "    1. **Specificity**: Does the question clearly identify a specific paper, model, or study, or specific topic, avoiding general references like 'the model' or 'the paper'? The question should mention specific details that clearly delineate which content is being queried, rather than asking broadly.\n",
    "    2. **Realism**: Is the question realistic and aligned with what students might genuinely ask in an academic setting? The question should reflect practical and common-sense inquiries likely to arise during study or review.\n",
    "    3. **Clarity**: Is the question clearly formulated, avoiding ambiguous language or phrasing that could confuse students? The question should be easy to understand and free from vague terms or complex structures.\n",
    "    Rate each criterion from 1 to 5, where:\n",
    "    1 - Very Poor\n",
    "    2 - Poor\n",
    "    3 - Fair\n",
    "    4 - Good\n",
    "    5 - Excellent\n",
    "    Question: \"{question}\"\n",
    "    Please provide a rating for each criterion along with a brief explanation.\n",
    "    Specificity:\n",
    "    Realism:\n",
    "    Clarity:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant that evaluates questions based on specificity, realism, and clarity.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "    except openai.RateLimitError:\n",
    "        logging.warning(\"Rate limit reached. Retrying...\")\n",
    "        raise\n",
    "    except openai.AuthenticationError:\n",
    "        logging.error(\"Authentication failed. Please check your API key.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in API call: {e}\")\n",
    "        return None\n",
    "\n",
    "    scores = {\n",
    "        \"Specificity\": None,\n",
    "        \"Realism\": None,\n",
    "        \"Clarity\": None\n",
    "    }\n",
    "    for line in response_text.splitlines():\n",
    "        for criterion in scores.keys():\n",
    "            if f\"{criterion}:\" in line:\n",
    "                try:\n",
    "                    scores[criterion] = int(line.split(\":\")[1].strip().split()[0])\n",
    "                except (ValueError, IndexError):\n",
    "                    logging.error(f\"Error parsing score for {criterion}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def process_questions(df, batch_size=1):\n",
    "    evaluated_questions_df = df.copy()\n",
    "    evaluated_questions_df['Specificity'] = None\n",
    "    evaluated_questions_df['Realism'] = None\n",
    "    evaluated_questions_df['Clarity'] = None\n",
    "    evaluated_questions_df['Average Score'] = None\n",
    "    evaluated_questions_df['Is Non-Specific'] = None\n",
    "\n",
    "    for i in tqdm(range(0, len(evaluated_questions_df), batch_size), desc=\"Processing questions\"):\n",
    "        batch = evaluated_questions_df.iloc[i:i+batch_size]\n",
    "        for index, row in batch.iterrows():\n",
    "            question = row['question']\n",
    "            is_non_specific = is_non_specific_question(question)\n",
    "            evaluated_questions_df.at[index, 'Is Non-Specific'] = is_non_specific\n",
    "            \n",
    "            scores = evaluate_question(question)\n",
    "            if scores:\n",
    "                valid_scores = [score for score in scores.values() if score is not None]\n",
    "                for criterion, score in scores.items():\n",
    "                    evaluated_questions_df.at[index, criterion] = score\n",
    "                if valid_scores:\n",
    "                    average_score = sum(valid_scores) / len(valid_scores)\n",
    "                    evaluated_questions_df.at[index, 'Average Score'] = average_score\n",
    "            \n",
    "            # Implement exponential backoff with jitter\n",
    "            time.sleep(random.uniform(5, 15))\n",
    "\n",
    "    return evaluated_questions_df\n",
    "\n",
    "# Verify API key is set\n",
    "print(f\"API key loaded: {'Yes' if api_key else 'No'}\")\n",
    "\n",
    "# Assuming qa_df is your DataFrame with columns: question_id, question, answer\n",
    "# If your DataFrame columns have different names, adjust accordingly\n",
    "\n",
    "# Process questions\n",
    "try:\n",
    "    evaluated_questions_df = process_questions(qa_df)\n",
    "\n",
    "    # Save the DataFrame with evaluations to a CSV file\n",
    "    evaluated_questions_df.to_csv('evaluated_questions_with_scores.csv', index=False)\n",
    "\n",
    "    # Display the evaluated questions with their scores\n",
    "    print(evaluated_questions_df)\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial phase of automatic evaluation, I will conduct a manual review of the questions. During this review, each question will be carefully assessed for its relevance, clarity, and alignment with the specified criteria. Based on this assessment, I will create a binary variable named “Keep.”\n",
    "\n",
    "In this system:\n",
    "\n",
    "\t•\tA value of 1.0 indicates that the question meets all quality standards and should be retained.\n",
    "\t•\tA value of 0.0 suggests that the question does not meet the necessary criteria and should be excluded from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Realism</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Average Score</th>\n",
       "      <th>Is Non-Specific</th>\n",
       "      <th>Keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29d3d7b9e48f046d75821fe3b763da05</td>\n",
       "      <td>What are some potential benefits of a new curr...</td>\n",
       "      <td>According to Bahaj and Reis (2023), one potent...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d804b24d051b1c5f06d0e0819f25ca37</td>\n",
       "      <td>How will students learn about different method...</td>\n",
       "      <td>After participating in this course, students w...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d3251ee436c00c1107189675b4bd8fc7</td>\n",
       "      <td>How will the research group DeSBi improve stat...</td>\n",
       "      <td>The research group DeSBi aims to develop novel...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e8de7463b8dde36dadddab419c363a57</td>\n",
       "      <td>What are some of the key findings in \"Debt Mat...</td>\n",
       "      <td>According to the study, one of the key finding...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ba886c5587fbbd26b9020e0c258a8ca7</td>\n",
       "      <td>What is the main focus of Prof. Dr. Joachim Ga...</td>\n",
       "      <td>The main focus of Prof. Dr. Joachim Gassen is ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  29d3d7b9e48f046d75821fe3b763da05   \n",
       "1  d804b24d051b1c5f06d0e0819f25ca37   \n",
       "2  d3251ee436c00c1107189675b4bd8fc7   \n",
       "3  e8de7463b8dde36dadddab419c363a57   \n",
       "4  ba886c5587fbbd26b9020e0c258a8ca7   \n",
       "\n",
       "                                            question  \\\n",
       "0  What are some potential benefits of a new curr...   \n",
       "1  How will students learn about different method...   \n",
       "2  How will the research group DeSBi improve stat...   \n",
       "3  What are some of the key findings in \"Debt Mat...   \n",
       "4  What is the main focus of Prof. Dr. Joachim Ga...   \n",
       "\n",
       "                                              answer  Specificity  Realism  \\\n",
       "0  According to Bahaj and Reis (2023), one potent...          2.0      4.0   \n",
       "1  After participating in this course, students w...          3.0      4.0   \n",
       "2  The research group DeSBi aims to develop novel...          4.0      5.0   \n",
       "3  According to the study, one of the key finding...          5.0      4.0   \n",
       "4  The main focus of Prof. Dr. Joachim Gassen is ...          3.0      4.0   \n",
       "\n",
       "   Clarity  Average Score  Is Non-Specific  Keep  \n",
       "0      3.0       3.000000            False   1.0  \n",
       "1      4.0       3.666667            False   1.0  \n",
       "2      4.0       4.333333            False   1.0  \n",
       "3      5.0       4.666667            False   1.0  \n",
       "4      4.0       3.666667            False   1.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluated_questions_df_checked = pd.read_csv('../assets/csv/evaluated_questions_with_scores_manual.csv', index_col=0)\n",
    "\n",
    "#keep only where column keep is 1\n",
    "evaluated_questions_df_checked = evaluated_questions_df_checked[evaluated_questions_df_checked['Keep'] == 1]\n",
    "evaluated_questions_df_checked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the steps to finalize the dataframe used for evaluating the retriever. Currently, the process incorporates a Human-in-the-Loop (HITL) approach to ensure the quality of the dataframe. This critical human review helps identify any discrepancies or issues that might not be caught by automated systems.\n",
    "\n",
    "Once a significant number of these dataframes have been reviewed and validated, it’s possible to develop a predictive model. This model would assess whether a question is likely to pass moderation based on the learned criteria. Implementing such a model could reduce or potentially eliminate the need for human moderation, streamlining the process to be more automatic.\n",
    "\n",
    "At this stage, human review is essential. With the rigorously checked dataframe, we can proceed to the evaluation phase of the retriever. This ensures that our retriever is tested under the most accurate and reliable conditions, paving the way for robust performance in real-world applications.\n",
    "\n",
    "        !! Makefile and script are stored in the src/retriver_evaluation!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
