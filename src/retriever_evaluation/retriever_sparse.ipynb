{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.7.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'recursive_chunker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cwd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../src\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     19\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cwd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../src/data_chunker\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrecursive_chunker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     24\u001b[0m pc \u001b[38;5;241m=\u001b[39m Pinecone(api_key\u001b[38;5;241m=\u001b[39mapi_key, environment\u001b[38;5;241m=\u001b[39menv)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recursive_chunker'"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import dotenv\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sys\n",
    "\n",
    "model = SentenceTransformer('Snowflake/snowflake-arctic-embed-l', trust_remote_code=True)\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "env = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the '../scripts' directory to the system path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(cwd, '../scripts')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(cwd, '../src')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(cwd, '../src/data_chunker')))\n",
    "\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=api_key, environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>html_content</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>56f56a63650a002093c08e80893d0507</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2021-11-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>course / short description General Management ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>83c06029a4d3f4df99d92f8db3036e69</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/academic-care...</td>\n",
       "      <td>2024-06-11</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>During the doctoral studies, there are various...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>1686e07fbe044704640183081b10ce89</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2024-07-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>The Finance Group @ Humboldt consists of those...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>363616941311823dd3e1b939f2c8b58a</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/academic-care...</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3661c6803d347e8d73af15a084bedd1b</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/academic-care...</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>The formal basis of doctoral studies is consti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  \\\n",
       "344  56f56a63650a002093c08e80893d0507   \n",
       "92   83c06029a4d3f4df99d92f8db3036e69   \n",
       "514  1686e07fbe044704640183081b10ce89   \n",
       "82   363616941311823dd3e1b939f2c8b58a   \n",
       "74   3661c6803d347e8d73af15a084bedd1b   \n",
       "\n",
       "                                                   url last_updated  \\\n",
       "344  https://www.wiwi.hu-berlin.de/en/Professorship...   2021-11-10   \n",
       "92   https://www.wiwi.hu-berlin.de/en/academic-care...   2024-06-11   \n",
       "514  https://www.wiwi.hu-berlin.de/en/Professorship...   2024-07-10   \n",
       "82   https://www.wiwi.hu-berlin.de/en/academic-care...   2021-11-30   \n",
       "74   https://www.wiwi.hu-berlin.de/en/academic-care...   2022-02-11   \n",
       "\n",
       "                                          html_content  \\\n",
       "344  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "92   <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "514  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "82   <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "74   <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "\n",
       "                                                  text  \n",
       "344  course / short description General Management ...  \n",
       "92   During the doctoral studies, there are various...  \n",
       "514  The Finance Group @ Humboldt consists of those...  \n",
       "82                                                 NaN  \n",
       "74   The formal basis of doctoral studies is consti...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone_text.sparse import BM25Encoder\n",
    "bm25 = BM25Encoder()\n",
    "\n",
    "df_subset = pd.read_csv('../../assets/csv/data_subset.csv', index_col=0)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'recursive_chunker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrecursive_chunker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recursive_chunker'"
     ]
    }
   ],
   "source": [
    "from recursive_chunker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00893313, -0.05035462,  0.00422336, ...,  0.02453731,\n",
       "       -0.01775656, -0.02322396], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "qa_df = pd.read_csv('../../assets/csv/qa_df.csv', index_col=0)\n",
    "question = qa_df['question'].iloc[0]\n",
    "question_embedded = model.encode(question)\n",
    "question_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>url</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>html_content</th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>general_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56f56a63650a002093c08e80893d0507_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2021-11-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>course / short description General Management ...</td>\n",
       "      <td>952.0</td>\n",
       "      <td>56f56a63650a002093c08e80893d0507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83c06029a4d3f4df99d92f8db3036e69_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/academic-care...</td>\n",
       "      <td>2024-06-11</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>During the doctoral studies, there are various...</td>\n",
       "      <td>1612.0</td>\n",
       "      <td>83c06029a4d3f4df99d92f8db3036e69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1686e07fbe044704640183081b10ce89_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2024-07-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>The Finance Group @ Humboldt consists of those...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1686e07fbe044704640183081b10ce89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1686e07fbe044704640183081b10ce89_2</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2024-07-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>Thu: 10 - 15 Brown, Felicia DOR 1, 304 2093 -...</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>1686e07fbe044704640183081b10ce89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3661c6803d347e8d73af15a084bedd1b_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/academic-care...</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>The formal basis of doctoral studies is consti...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3661c6803d347e8d73af15a084bedd1b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            unique_id  \\\n",
       "0  56f56a63650a002093c08e80893d0507_1   \n",
       "1  83c06029a4d3f4df99d92f8db3036e69_1   \n",
       "2  1686e07fbe044704640183081b10ce89_1   \n",
       "3  1686e07fbe044704640183081b10ce89_2   \n",
       "5  3661c6803d347e8d73af15a084bedd1b_1   \n",
       "\n",
       "                                                 url last_updated  \\\n",
       "0  https://www.wiwi.hu-berlin.de/en/Professorship...   2021-11-10   \n",
       "1  https://www.wiwi.hu-berlin.de/en/academic-care...   2024-06-11   \n",
       "2  https://www.wiwi.hu-berlin.de/en/Professorship...   2024-07-10   \n",
       "3  https://www.wiwi.hu-berlin.de/en/Professorship...   2024-07-10   \n",
       "5  https://www.wiwi.hu-berlin.de/en/academic-care...   2022-02-11   \n",
       "\n",
       "                                        html_content  \\\n",
       "0  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "1  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "2  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "3  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "5  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "\n",
       "                                                text     len  \\\n",
       "0  course / short description General Management ...   952.0   \n",
       "1  During the doctoral studies, there are various...  1612.0   \n",
       "2  The Finance Group @ Humboldt consists of those...  2000.0   \n",
       "3   Thu: 10 - 15 Brown, Felicia DOR 1, 304 2093 -...  1017.0   \n",
       "5  The formal basis of doctoral studies is consti...  2000.0   \n",
       "\n",
       "                         general_id  \n",
       "0  56f56a63650a002093c08e80893d0507  \n",
       "1  83c06029a4d3f4df99d92f8db3036e69  \n",
       "2  1686e07fbe044704640183081b10ce89  \n",
       "3  1686e07fbe044704640183081b10ce89  \n",
       "5  3661c6803d347e8d73af15a084bedd1b  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_stats, char_df = process_data_tokens(df_subset,[2000])\n",
    "char_df = pd.DataFrame(char_df)\n",
    "char_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 952/952 [00:00<00:00, 35025.50it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 58\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_df\n\u001b[1;32m     56\u001b[0m df_subset\n\u001b[0;32m---> 58\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[43membed_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 42\u001b[0m, in \u001b[0;36membed_dataframe\u001b[0;34m(data, embed_model)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mGenerates embeddings for the text content of each row in the input DataFrame.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m                  the generated embeddings. Rows where embedding failed are removed.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEmbedding texts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         embedding \u001b[38;5;241m=\u001b[39m get_embeddings(embed_model, [row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]])[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "char_df['text'].iloc[0]\n",
    "\n",
    "bm25.fit(char_df['text'].iloc[0])\n",
    "bm25.encode_documents(char_df['text'].iloc[0])\n",
    "\n",
    "import tqdm\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "def get_embeddings(embed_model: Any, texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    A wrapper function to get embeddings from different types of models.\n",
    "\n",
    "    Args:\n",
    "        embed_model (Any): The embedding model (either HuggingFaceEmbeddings or a model with encode_documents method).\n",
    "        texts (List[str]): A list of texts to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: A list of embeddings.\n",
    "    \"\"\"\n",
    "    if hasattr(embed_model, 'embed_documents'):\n",
    "        return embed_model.embed_documents(texts)\n",
    "    elif hasattr(embed_model, 'encode'):\n",
    "        return embed_model.encode(texts)\n",
    "    else:\n",
    "        raise AttributeError(\"The provided model doesn't have 'embed_documents' or 'encode' method.\")\n",
    "\n",
    "\n",
    "def embed_dataframe(data: pd.DataFrame, embed_model: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates embeddings for the text content of each row in the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame containing at least a 'text' column and a\n",
    "                             'unique_id' column for error reporting.\n",
    "        embed_model (Any): An embedding model object that has either 'embed_documents' or 'encode' method.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with an additional 'embedding' column containing\n",
    "                      the generated embeddings. Rows where embedding failed are removed.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Embedding texts\"):\n",
    "        try:\n",
    "            embedding = get_embeddings(embed_model, [row['text']])[0]\n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            \n",
    "            embeddings.append(None)\n",
    "    \n",
    "    data['embedding'] = embeddings\n",
    "    result_df = data.dropna(subset=['embedding'])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "df_subset\n",
    "\n",
    "embedded = embed_dataframe(df_subset, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   0%|          | 1/537 [00:00<03:51,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 56f56a63650a002093c08e80893d0507_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   1%|          | 3/537 [00:00<02:28,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 83c06029a4d3f4df99d92f8db3036e69_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 1686e07fbe044704640183081b10ce89_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   1%|          | 5/537 [00:01<01:47,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 1686e07fbe044704640183081b10ce89_2: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 3661c6803d347e8d73af15a084bedd1b_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   1%|▏         | 7/537 [00:01<01:30,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 3661c6803d347e8d73af15a084bedd1b_2: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 29d3d7b9e48f046d75821fe3b763da05_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   2%|▏         | 9/537 [00:01<01:16,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 29d3d7b9e48f046d75821fe3b763da05_2: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 517bc48ef5ecfce43883a37f2031dba8_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   2%|▏         | 11/537 [00:02<01:29,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_2: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   2%|▏         | 13/537 [00:02<01:34,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_3: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_4: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   3%|▎         | 15/537 [00:02<01:37,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_5: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_6: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   3%|▎         | 17/537 [00:03<01:40,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_7: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_8: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   4%|▎         | 19/537 [00:03<01:40,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_9: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 7aa6df98ab70502b9585b87adbb30219_10: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID c3231249e7654032c360bfdcdcc620f1_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID b9beed4cb268b0165607de6747ae3b07_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   4%|▍         | 23/537 [00:04<01:09,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID a0e3ddbb86db4eae9f33ef552f970cd7_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID a0e3ddbb86db4eae9f33ef552f970cd7_2: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   5%|▍         | 25/537 [00:04<01:10,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID a0e3ddbb86db4eae9f33ef552f970cd7_3: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID a0e3ddbb86db4eae9f33ef552f970cd7_4: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   5%|▍         | 26/537 [00:04<01:11,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID a0e3ddbb86db4eae9f33ef552f970cd7_5: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID a0e3ddbb86db4eae9f33ef552f970cd7_6: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   5%|▌         | 29/537 [00:04<01:12,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID fa30a5135dae4189bcf1cd5831072475_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID fa30a5135dae4189bcf1cd5831072475_2: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   6%|▌         | 31/537 [00:05<01:23,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID fa30a5135dae4189bcf1cd5831072475_3: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID fa30a5135dae4189bcf1cd5831072475_4: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   6%|▌         | 33/537 [00:05<01:28,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID fa30a5135dae4189bcf1cd5831072475_5: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID fa30a5135dae4189bcf1cd5831072475_6: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   7%|▋         | 35/537 [00:05<01:13,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID c362edd089f90612afed6f245a685bcb_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 73ef3b2a3f4f1e3e2aebcb96c66d6fe3_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   7%|▋         | 36/537 [00:06<01:12,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 1a4615b51de4134f65211ee4285f8c63_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 0eb477222e891ebe2a0cddc58ecbe018_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   7%|▋         | 38/537 [00:06<01:05,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID dc855dc9768c3d1ceee22b09a6701dc8_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID dc855dc9768c3d1ceee22b09a6701dc8_2: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   8%|▊         | 41/537 [00:06<01:10,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_2: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   8%|▊         | 43/537 [00:07<01:21,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_3: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_4: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   8%|▊         | 45/537 [00:07<01:28,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_5: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_6: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   9%|▊         | 46/537 [00:07<01:29,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_7: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 5cbc7a0eae028d9788258a9cd38f7305_8: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   9%|▉         | 49/537 [00:08<01:20,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embedding failed for ID 3b6c223c6c74109b6b905ff96bec92a1_1: unhashable type: 'dict'\n",
      "Sparse embedding failed for ID 8aaf51e313f7b593d2b4106fa21037a5_1: unhashable type: 'dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_df\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Usage Example\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m embedded_char \u001b[38;5;241m=\u001b[39m \u001b[43membed_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 26\u001b[0m, in \u001b[0;36membed_dataframe\u001b[0;34m(data, embed_model, sparse_model)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(data\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding texts\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Get dense embedding\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         dense_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m         dense_embeddings\u001b[38;5;241m.\u001b[39mappend(dense_embedding)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[68], line 23\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(embed_model, texts)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(embed_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencode\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided model doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_documents\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:371\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    368\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 371\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m truncate_embeddings(\n\u001b[1;32m    373\u001b[0m         out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate_dim\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 551\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/huber/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "bm25 = BM25Encoder()\n",
    "\n",
    "model = SentenceTransformer('Snowflake/snowflake-arctic-embed-l', trust_remote_code=True)\n",
    "\n",
    "\n",
    "def embed_dataframe(data: pd.DataFrame, embed_model: Any, sparse_model: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates embeddings (both dense and sparse) for the text content of each row in the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame containing at least a 'text' column and a\n",
    "                             'unique_id' column for error reporting.\n",
    "        embed_model (Any): An embedding model object that has either 'embed_documents' or 'encode' method.\n",
    "        sparse_model (Any): A model object that generates sparse embeddings, such as BM25.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with additional 'embedding' and 'sparse_embedding' columns containing\n",
    "                      the generated embeddings. Rows where embedding failed are removed.\n",
    "    \"\"\"\n",
    "    dense_embeddings = []\n",
    "    sparse_embeddings = []\n",
    "    \n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Embedding texts\"):\n",
    "        try:\n",
    "            # Get dense embedding\n",
    "            dense_embedding = get_embeddings(embed_model, [row['text']])[0]\n",
    "            dense_embeddings.append(dense_embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Dense embedding failed for ID {row['unique_id']}: {str(e)}\")\n",
    "            dense_embeddings.append(np.nan)  # Handle dense embedding failure\n",
    "\n",
    "        try:\n",
    "            # Get sparse embedding\n",
    "            sparse_embedding = dict(Counter(sparse_model.encode_documents([row['text']]))[0]\n",
    "            sparse_embeddings.append(sparse_embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Sparse embedding failed for ID {row['unique_id']}: {str(e)}\")\n",
    "            sparse_embeddings.append(np.nan)  # Handle sparse embedding failure\n",
    "\n",
    "    # Add the embeddings to the DataFrame\n",
    "    data['dense_embedding'] = dense_embeddings\n",
    "    data['sparse_embedding'] = sparse_embeddings\n",
    "    \n",
    "    # Drop rows where either embedding failed (i.e., embeddings are NaN)\n",
    "    result_df = data.dropna(subset=['dense_embedding', 'sparse_embedding'])\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Usage Example\n",
    "embedded_char = embed_dataframe(char_df, model, bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'indices': ['i', 'n', 'd', 'c', 'e', 's'], 'values': [2, 1, 1, 1, 1, 1]},\n",
       " {'indices': ['v', 'a', 'l', 'u', 'e', 's'], 'values': [1, 1, 1, 1, 1, 1]}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "test = embedded_char['sparse_embedding'].iloc[0]\n",
    "\n",
    "def build_dict(input_batch):\n",
    " # store a batch of sparse embeddings\n",
    "   sparse_emb = []\n",
    "   # iterate through input batch\n",
    "   for token_ids in input_batch:\n",
    "       indices = []\n",
    "       values = []\n",
    "       # convert the input_ids list to a dictionary of key to frequency values\n",
    "       d = dict(Counter(token_ids))\n",
    "       for idx in d:\n",
    "            indices.append(idx)\n",
    "            values.append(d[idx])\n",
    "       sparse_emb.append({'indices': indices, 'values': values})\n",
    "   # return sparse_emb list\n",
    "   return sparse_emb\n",
    "\n",
    "build_dict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>url</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>html_content</th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>general_id</th>\n",
       "      <th>dense_embedding</th>\n",
       "      <th>sparse_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56f56a63650a002093c08e80893d0507_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2021-11-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>course / short description General Management ...</td>\n",
       "      <td>952.0</td>\n",
       "      <td>56f56a63650a002093c08e80893d0507</td>\n",
       "      <td>[-0.0006726693, -0.047064904, -0.00024737045, ...</td>\n",
       "      <td>{'indices': [192565064, 883390095, 3071667970,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83c06029a4d3f4df99d92f8db3036e69_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/academic-care...</td>\n",
       "      <td>2024-06-11</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>During the doctoral studies, there are various...</td>\n",
       "      <td>1612.0</td>\n",
       "      <td>83c06029a4d3f4df99d92f8db3036e69</td>\n",
       "      <td>[-0.007625542, -0.027227053, 0.035203204, -0.0...</td>\n",
       "      <td>{'indices': [1334991813, 4144206424, 215942184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1686e07fbe044704640183081b10ce89_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2024-07-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>The Finance Group @ Humboldt consists of those...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1686e07fbe044704640183081b10ce89</td>\n",
       "      <td>[0.043651793, -0.026485657, 0.023746977, 0.003...</td>\n",
       "      <td>{'indices': [3310569130, 1021187622, 315661618...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1686e07fbe044704640183081b10ce89_2</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/Professorship...</td>\n",
       "      <td>2024-07-10</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>Thu: 10 - 15 Brown, Felicia DOR 1, 304 2093 -...</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>1686e07fbe044704640183081b10ce89</td>\n",
       "      <td>[-0.00072776823, -0.10089667, -0.0037318603, -...</td>\n",
       "      <td>{'indices': [1045259396, 2263091519, 359766348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3661c6803d347e8d73af15a084bedd1b_1</td>\n",
       "      <td>https://www.wiwi.hu-berlin.de/en/academic-care...</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.or...</td>\n",
       "      <td>The formal basis of doctoral studies is consti...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3661c6803d347e8d73af15a084bedd1b</td>\n",
       "      <td>[0.004780789, -0.05352055, 0.04896739, 0.01004...</td>\n",
       "      <td>{'indices': [3168814557, 1772413527, 133499181...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            unique_id  \\\n",
       "0  56f56a63650a002093c08e80893d0507_1   \n",
       "1  83c06029a4d3f4df99d92f8db3036e69_1   \n",
       "2  1686e07fbe044704640183081b10ce89_1   \n",
       "3  1686e07fbe044704640183081b10ce89_2   \n",
       "5  3661c6803d347e8d73af15a084bedd1b_1   \n",
       "\n",
       "                                                 url last_updated  \\\n",
       "0  https://www.wiwi.hu-berlin.de/en/Professorship...   2021-11-10   \n",
       "1  https://www.wiwi.hu-berlin.de/en/academic-care...   2024-06-11   \n",
       "2  https://www.wiwi.hu-berlin.de/en/Professorship...   2024-07-10   \n",
       "3  https://www.wiwi.hu-berlin.de/en/Professorship...   2024-07-10   \n",
       "5  https://www.wiwi.hu-berlin.de/en/academic-care...   2022-02-11   \n",
       "\n",
       "                                        html_content  \\\n",
       "0  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "1  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "2  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "3  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "5  <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.or...   \n",
       "\n",
       "                                                text     len  \\\n",
       "0  course / short description General Management ...   952.0   \n",
       "1  During the doctoral studies, there are various...  1612.0   \n",
       "2  The Finance Group @ Humboldt consists of those...  2000.0   \n",
       "3   Thu: 10 - 15 Brown, Felicia DOR 1, 304 2093 -...  1017.0   \n",
       "5  The formal basis of doctoral studies is consti...  2000.0   \n",
       "\n",
       "                         general_id  \\\n",
       "0  56f56a63650a002093c08e80893d0507   \n",
       "1  83c06029a4d3f4df99d92f8db3036e69   \n",
       "2  1686e07fbe044704640183081b10ce89   \n",
       "3  1686e07fbe044704640183081b10ce89   \n",
       "5  3661c6803d347e8d73af15a084bedd1b   \n",
       "\n",
       "                                     dense_embedding  \\\n",
       "0  [-0.0006726693, -0.047064904, -0.00024737045, ...   \n",
       "1  [-0.007625542, -0.027227053, 0.035203204, -0.0...   \n",
       "2  [0.043651793, -0.026485657, 0.023746977, 0.003...   \n",
       "3  [-0.00072776823, -0.10089667, -0.0037318603, -...   \n",
       "5  [0.004780789, -0.05352055, 0.04896739, 0.01004...   \n",
       "\n",
       "                                    sparse_embedding  \n",
       "0  {'indices': [192565064, 883390095, 3071667970,...  \n",
       "1  {'indices': [1334991813, 4144206424, 215942184...  \n",
       "2  {'indices': [3310569130, 1021187622, 315661618...  \n",
       "3  {'indices': [1045259396, 2263091519, 359766348...  \n",
       "5  {'indices': [3168814557, 1772413527, 133499181...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_documents(df: pd.DataFrame, chunk_size: int, doc_type: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generates a list of document dictionaries from the input DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing embedded text chunks.\n",
    "        chunk_size (int): The chunk size used in chunking.\n",
    "        doc_type (str): The type of document ('sentence', 'token', or 'semantic').\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of document dictionaries.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Debugging: Print row contents\n",
    "        #print(f\"Processing row: {row}\")\n",
    "        \n",
    "        # Use get() method with a default value to avoid KeyError\n",
    "        unique_id = row.get('unique_id', str(uuid.uuid4()))\n",
    "        \n",
    "        # Prioritize 'id' over 'general_id' for consistency\n",
    "        general_id = row.get('id', row.get('general_id', 'unknown'))\n",
    "        \n",
    "        # Debugging: Print ID fields\n",
    "        #print(f\"unique_id: {unique_id}, general_id: {general_id}\")\n",
    "        \n",
    "        document = {\n",
    "            \"unique_id\": unique_id,\n",
    "            'values': row.get(\"embedding\", []),\n",
    "            \"metadata\": {\n",
    "                \"url\": row.get(\"url\", \"\"),\n",
    "                \"text\": row.get(\"text\", \"\"),\n",
    "                'general_id': general_id,\n",
    "                'chunk_size': chunk_size,\n",
    "                'doc_type': doc_type\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add 'last_updated' to metadata if it exists in the DataFrame\n",
    "        if 'last_updated' in row:\n",
    "            document['metadata']['date'] = row['last_updated']\n",
    "        \n",
    "        documents.append(document)\n",
    "    \n",
    "    return documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
